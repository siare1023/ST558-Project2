---
title: "ST558 - Project 2"
author: "Group 5: Tyler Pollard & Lucy Yin"
output: 
 github_document:
   toc: true
   toc_depth: 3
params: 
      weekday: monday
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE)
```
# Required Packages
```{r packages, echo=FALSE}
library(tidyverse)
library(dplyr)
library(knitr)
library(ggplot2)
library(caret)
library(gridExtra)
library(corrplot)
library(GGally)
```

# Introduction
For this report we will be using 6 models (4 linear regression, 1 random forest model, 1 boosted tree model) to make predictions on the total count of bike riders using data from the Bike Sharing Dataset (dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)). This dataset contains hourly and daily count of registered, casual, and total sum of riders in the Capital bikeshare system, contributing variables include:  
  
* season (winter, spring, summer, fall)  
* year (2011, 2012)  
* month of the year 
* hour of the day  
* holiday (yes, no)  
* day of the week  
* working day (yes or no)  
* weather situation (mostly clear, mist, light precipitation, heavy precipitation)  
* temperature  
* feeling temperature  
* humidity  
* wind speed  

There are 3 different types of response variables in the dataset:  
  
* registered: registered riders who uses this bikeshare service regularly  
* casual: un-registered riders who use this service casually or on occasions  
* total: combined count of registered and casual riders  
  
For our analysis, we will be working with almost all of the variables as predictors, and our response variable will be the total count of bike riders.  
  
We will be selecting predictors using the `step()` function which chooses a model by AIC in a stepwise algorithm. As a result, which predictors we incorporate in our linear regression models and ensemble tree (specifically random forest and boosted tree) models may differ depending on which day of the week we look at. We'll randomly split the data into training and test sets and fit the 6 models on the training set. Ultimately we will fit the 6 models on the test set and decide on which model produced the best prediction, which we judge by the smallest root mean squared error value.  
    
# Data
First we will read in both the `hours.csv` and `day.csv` data. 
```{r data - read in}
# read in data
hour.data <- read_csv("data/hour.csv") %>% as_tibble()
day.data <- read_csv("data/day.csv") %>% as_tibble()
```

We will make corrections on variable types, specifically we're making sure categorical variables will be appropriately classified as factors with clear levels. 
```{r data - change type}
# correct the variable types
hour.data$season <- factor(hour.data$season)
levels(hour.data$season) <- list(winter = 1, spring = 2, summer = 3, fall = 4)

hour.data$yr <- factor(hour.data$yr)
levels(hour.data$yr) <- list("2011" = 0, "2012" = 1)

hour.data$weekday <- factor(hour.data$weekday)
levels(hour.data$weekday) <- list(monday = 1, tuesday = 2, wednesday = 3, thursday = 4, friday = 5, saturday = 6, sunday = 0)

hour.data$mnth <- factor(hour.data$mnth)
hour.data$hr <- factor(hour.data$hr)
hour.data$holiday <- factor(hour.data$holiday)
hour.data$workingday <- factor(hour.data$workingday)
hour.data$weathersit <- factor(hour.data$weathersit)

day.data$season <- factor(day.data$season)
levels(day.data$season) <- list(winter = 1, spring = 2, summer = 3, fall = 4)

day.data$yr <- factor(day.data$yr)
levels(day.data$yr) <- list("2011" = 0, "2012" = 1)

day.data$weekday <- factor(day.data$weekday)
levels(day.data$weekday) <- list(monday = 1, tuesday = 2, wednesday = 3, thursday = 4, friday = 5, saturday = 6, sunday = 0)

day.data$mnth <- factor(day.data$mnth)
day.data$holiday <- factor(day.data$holiday)
day.data$workingday <- factor(day.data$workingday)
day.data$weathersit <- factor(day.data$weathersit)
```

Because the variables temperature, feeling temperature, humidity and windspeed are normalized according to different measures, we will un-normalize them and save the raw values as separate columns in the dataset. 
```{r data - unnormalize variables}
# Temp Unnormal
temp.tmin = -8
temp.tmax = 39
hour.data$temp.unnormal <- hour.data$temp*(temp.tmax - temp.tmin) + temp.tmin # Unnormalize temp
hour.data$temp.F <- hour.data$temp.unnormal*(9/5) + 32 # Convert to Fahrenheit
day.data$temp.unnormal <- hour.data %>% group_by(dteday) %>% summarise(mean = mean(temp.unnormal)) %>% select(mean)
day.data$temp.unnormal <- day.data$temp.unnormal[[1]]
day.data$temp.F <- hour.data %>% group_by(dteday) %>% summarise(mean = mean(temp.F)) %>% select(mean)
day.data$temp.F <- day.data$temp.F[[1]]

# Atemp Unnormal
atemp.tmin = -16
atemp.tmax = 50
hour.data$atemp.unnormal <- hour.data$atemp*(atemp.tmax - atemp.tmin) + atemp.tmin # Unnormalize atemps
hour.data$atemp.F <- hour.data$atemp.unnormal*(9/5) + 32 # Convert to Fahrenheit
day.data$atemp.unnormal <- hour.data %>% group_by(dteday) %>% summarise(mean = mean(atemp.unnormal)) %>% select(mean)
day.data$atemp.unnormal <- day.data$atemp.unnormal[[1]]
day.data$atemp.F <- hour.data %>% group_by(dteday) %>% summarise(mean = mean(atemp.F)) %>% select(mean)
day.data$atemp.F <- day.data$atemp.F[[1]]

# Humidity Unnormal
day.data$hum.unnormal <- day.data$hum * 100
hour.data$hum.unnormal <- hour.data$hum * 100

# Windspeed Unnormal
day.data$windspeed.unnormal <- day.data$windspeed * 67
hour.data$windspeed.unnormal <- hour.data$windspeed * 67
```

Because hour and day data are stored separately, we create a `total.data` table with all the information combined just in case we need to access this in later steps. 
```{r data - create total data}
# add in a new variable before merging
hour.data <- mutate(hour.data, type = "hour")
day.data <- mutate(day.data, type = "day", hr = NA) %>% select(instant, dteday, season, yr, mnth, hr, everything())

# merge to create complete list of hour/day data
total.data <- rbind(hour.data, day.data)
```

We will filter to only include data from one specific day of the week at a time. 
```{r data - filter parameter}
# filter out to one specific day of the week
hour.data <- hour.data %>% filter(weekday == params$weekday)
day.data <- day.data %>% filter(weekday == params$weekday)
total.data <- total.data %>% filter(weekday == params$weekday)
```

We randomly sample from the filtered data to form a training set (with 70% of data) and test set (with the remaining 30% of data). Here we randomly sampled from the day dataset and split it into training and test sets, then we split the corresponding data from the hours dataset according to which days are in the training set and which days are in the test set. We will be using the hours dataset for our modeling, but we wanted to make sure our day and hour datasets had matching training and test splits. 
```{r data - split training/test}
# splitting data into training & test sets
set.seed(7)
train <- sample(1:nrow(day.data), size = nrow(day.data)*0.7)
test <- dplyr::setdiff(1:nrow(day.data), train)
day.training.data <- day.data[train, ]
day.test.data <- day.data[test, ]

hour.training.data <- hour.data[hour.data$dteday %in% day.training.data$dteday,]
hour.test.data <- hour.data[hour.data$dteday %in% day.test.data$dteday,]
```

# Summarization
We have some basic summary statistics and plots about our training data. 

## Contingency Tables  
### Weather Situation
Below is a contingency table that shows the count of days that fall into the different categories of weather situation. This table will help justify the total count of riders because it can be expected that the number of casual riders, which influences the total count of riders, will be higher on nicer days that fall into the first two categories of Mostly clear and Mist.

```{r contigency table - weathersit}
levels(day.training.data$weathersit) <- list(
  "Mostly clear" = "1",
  "Mist" = "2",
  "Light precipitation" = "3",
  "Heavy precipitation" = "4")
kable(t(table(day.training.data$weathersit)))
```

### Year, Season and Count of Riders
These contingency tables show what count range of riders utilized the bikeshare service for a given season or for a given year. This table can help us see if the number of riders increased/decreased from 2011 to 2012, or if season has an effect on how many riders used the bikeshare service. 
```{r contingency table - year,season,count}
kable(table(day.training.data$season, cut(day.training.data$cnt, breaks = 2, dig.lab = 10)), caption = "Occurrences of # Range of Riders of a given Season")
kable(table(day.training.data$yr, cut(day.training.data$cnt, breaks = 2, dig.lab = 10)), caption = "Occurrences of # Range of Riders of a given Year")
```

### Working Day and Count of Casual Riders
This contingency table show what count range of casual riders utilized the bikeshare service on working day versus non-working day. Intuitively we'd suspect that there would more casual riders on non-working day than working day, this table can show us whether it's true or not.  
```{r contingency table - workingday,casual}
levels(day.training.data$workingday) <- list("workday" = 1, "non-workday" = 0)
kable(table(day.training.data$workingday, cut(day.training.data$casual, breaks = 2, dig.lab = 10)), caption = "Occurrences of # Range of Casual Riders of Workday vs. non-Workday")
```

## Summary Tables  
### Feeling Temperature
The summary tables of feeling temperature show the 5 number summary along with the mean and standard deviation of what the temperature actually felt like over the different years. The summary table for both the normalized and raw feeling temperatures are provided. These tables give insight to the range of feeling temperatures felt by the riders for the different years.
```{r summary table - atemp}
# Normalized feeling temperature
atemp.summary <- hour.training.data %>% group_by(yr) %>% summarise(Min. = min(atemp), `1st Qu.` = quantile(atemp,0.25), Median = median(atemp), Mean = mean(atemp), `3rd Qu.` = quantile(atemp, 0.75), Max. = max(atemp), `St. Dev.` = sd(atemp))
kable(atemp.summary, digits = 2, caption = "Summary of feeling temperatures by year")

# Raw feeling temperature in Fahrenheit
atemp.summary.unnormal <- hour.training.data %>% group_by(yr) %>% summarise(Min. = min(atemp.F), `1st Qu.` = quantile(atemp.F,0.25), Median = median(atemp.F), Mean = mean(atemp.F), `3rd Qu.` = quantile(atemp.F, 0.75), Max. = max(atemp.F), `St. Dev.` = sd(atemp.F))
kable(atemp.summary.unnormal, digits = 2, caption = "Summary of raw feeling temperatures by year")
```

### Humidity
These summary tables show the spread of normalized and raw humidity values. The table includes the 5 number summary along with mean and standard deviation, which gives insight to the range of humidity levels riders experienced. 
```{r summary table - humidity}
kable(t(c(summary(day.training.data$hum), St.Dev. = sd(day.training.data$hum))), digits = 2, caption = "Summary of Normalized Humidity")
kable(t(c(summary(day.training.data$hum.unnormal), St.Dev. = sd(day.training.data$hum.unnormal))), digits = 2, caption = "Summary of Raw Humidity")  
```

### Wind Speed
These summary tables show the spread of normalized and raw wind speeds. The table includes the 5 number summary along with mean and standard deviation, which gives insight to the range of wind speeds riders experienced. 
```{r summary table - windspeed}
kable(t(c(summary(day.training.data$windspeed), St.Dev. = sd(day.training.data$windspeed))), digits = 2, caption = "Summary of Normalized Wind Speed")
kable(t(c(summary(day.training.data$windspeed.unnormal), St.Dev. = sd(day.training.data$windspeed.unnormal))), digits = 2, caption = "Summary of Raw Wind Speed ")
```

## Histograms
### Humidity and Windspeed Distributions
The following density plots show the distribution of the weather effects for raw humidity and raw wind speed over the span of the biker data. These distributions provide insight on what values for each weather effect can be expected and how the combination of each effect may drive the different weather situations and in turn the expected count of riders.
```{r histogram - count}
hum.histogram <- ggplot(data = day.training.data, aes(x = hum.unnormal)) + 
  geom_histogram(aes(y = ..density..), bins = 30) + 
  geom_density(color = "red", size = 2) + 
  labs(title = "Humidity Distribution", x = "Raw Humidity", y = "Density")
windspeed.histogram <- ggplot(data = day.training.data, aes(x = windspeed.unnormal)) + 
  geom_histogram(aes(y = ..density..), bins = 30) + 
  geom_density(color = "red", size = 2) + 
  labs(title = "Windspeed Distribution", x = "Raw Windspeed", y = "Density")
grid.arrange(hum.histogram, windspeed.histogram, ncol = 2, top = "Density Distribution of Weather Effects")
```

## Density Plot
### Casual Riders and Weather Situation
This density plot shows the amount of casual riders in a given weather situation. Intuitively we suspect that there would more casual riders in better weather conditions. This density plot can show us whether or not this is true. 
```{r density plot - weather,casual}
ggplot(hour.training.data, aes(x = casual)) + 
  geom_density(alpha = 0.5, position = "stack", aes(fill = weathersit)) + 
  labs(title = "Density plot of casual riders",
       subtitle = "Specified by weather situation",
       x = "Count of Casual Riders",
       y = "Density") + 
  scale_fill_discrete(name = "Weather Situation", labels = c("Mostly Clear", "Mist", "Light Precip.", "Heavy Precip."))  
```

### Registered Riders and Weather Situation
This density plot shows the amount of registered riders in a given weather situation. We suspect that the amount of registered riders wouldn't be as affected by weather situation as the amount of casual riders would. This density plot can show us whether or not this is true. 
```{r density plot - weather,registered}
ggplot(hour.training.data, aes(x = registered)) + 
  geom_density(alpha = 0.5, position = "stack", aes(fill = weathersit)) + 
  labs(title = "Density plot of casual riders",
       subtitle = "Specified by weather situation",
       x = "Count of Registered Riders",
       y = "Density") + 
  scale_fill_discrete(name = "Weather Situation", labels = c("Mostly Clear", "Mist", "Light Precip.", "Heavy Precip."))  
```

### Total Riders and Weather Situation
This density plot shows the total count of riders in a given weather situation. We may see a relationship between how many riders there are and what type of weather condition it is. 
```{r density plot - weather,cnt}
ggplot(hour.training.data, aes(x = cnt)) + 
  geom_density(alpha = 0.5, position = "stack", aes(fill = weathersit)) + 
  labs(title = "Density plot of casual riders",
       subtitle = "Specified by weather situation",
       x = "Total Count of Riders",
       y = "Density") + 
  scale_fill_discrete(name = "Weather Situation", labels = c("Mostly Clear", "Mist", "Light Precip.", "Heavy Precip."))  
```

### Casual Riders and Holiday
This density plot shows the amount of casual riders depending on whether it is a holiday or non-holiday. We suspect there would be more casual riders on holidays, especially at larger counts. This density plot can show us whether that is true. 
```{r}
ggplot(hour.training.data, aes(x = casual)) + 
  geom_density(alpha = 0.5, position = "stack", aes(fill = holiday)) +
  labs(title = "Density plot of casual riders",
       subtitle = "Specified by holiday or not",
       x = "Count of Casual Riders",
       y = "Density") +
  scale_fill_discrete(name = "Holiday?", labels = c("Non-Holiday", "Holiday"))    
```

### Registered Riders and Holiday
This density plot shows the amount of registered riders depending on whether it is a holiday or non-holiday. We suspect there would be more registered riders on non-holidays, especially at larger counts. This density plot can show us whether that is true. 
```{r density plot - holiday,registered}
ggplot(hour.training.data, aes(x = registered)) + 
  geom_density(alpha = 0.5, position = "stack", aes(fill = holiday)) +
  labs(title = "Density plot of casual riders",
       subtitle = "Specified by holiday or not",
       x = "Count of Registered Riders",
       y = "Density") +
  scale_fill_discrete(name = "Holiday?", labels = c("Non-Holiday", "Holiday"))    
```

### Total Riders and Holiday
This density plot shows the total count of riders depending on whether it is a holiday or non-holiday. This plot could show a relationship between the amount of riders versus whether it's a holiday or non-holiday.  
```{r density plot - holiday,cnt}
ggplot(hour.training.data, aes(x = cnt)) + 
  geom_density(alpha = 0.5, position = "stack", aes(fill = holiday)) +
  labs(title = "Density plot of casual riders",
       subtitle = "Specified by holiday or not",
       x = "Total Count of Riders",
       y = "Density") +
  scale_fill_discrete(name = "Holiday?", labels = c("Non-Holiday", "Holiday"))    
```

## Boxplots  
### Feeling Temperature Over the Year
To get a better understanding of the feeling temperature spreads over the year, boxplots of the feeling temperature are plotted by month with the data points for each day used to create them plotted overtop. Intuitively, it can be expected that the feeling temperature rises from the beginning of the year into the middle of summer and then drops back down over the fall and winter months. These boxplots provide insight into the possible number of rider fluctuation over the different months of the year.

```{r boxplot- adjusted temperature, fig.width = 8}
atemp.boxplot.df <- day.training.data
levels(atemp.boxplot.df$mnth) <- list(January = 1, February = 2, March = 3, April = 4, May = 5, June = 6, July = 7, August = 8, September = 9, October = 10, November = 11, December = 12)
ggplot(data = atemp.boxplot.df, aes(x = mnth, y = atemp.F)) + 
  geom_boxplot() + 
  geom_point(position = "jitter", color = "blue") + 
  labs(title = "Feeling temperature distribution per month", x = "Month", y = "Feeling Temperature (F)")
```

### Riders of Every Hour and Weather Situation  
This boxplot shows the 5 number summary (in boxplot form with occasional outliers) of the amount of riders for each hour of the day. The colored lines should the mean number of riders for each given weather situation. We expect that the highest amount of riders should appear around the morning and afternoon commute time given it's not on a holiday or on the weekends. This boxplot can show if that's true. 
```{r boxplot - weathersit,count,hour}
ggplot(hour.training.data, aes(x = hr, y = cnt)) + 
  geom_boxplot() + 
  stat_summary(fun = mean, geom = "line", lwd = 0.8, aes(group = weathersit, col = weathersit)) + 
  labs(title = "Count of riders for every hr",
       subtitle = "Mean values based on weather situation",
       x = "Hour of the Day",
       y = "Count of Riders") + 
  scale_color_discrete(name = "Weather Situation", labels = c("Mostly Clear", "Mist", "Light Precip.", "Heavy Precip."))
```

## Scatter Plots  
### Count vs Casual by Season  
These four scatter plots show the relation between the total number of riders and casual riders by day with linear models plotted overtop parsed by season. These plots show how the number of casual riders contribute to the total count of riders for each season. The greater the slope of the linear model correlates to a greater number of causal riders contributing to the total count of riders.
```{r scatter - registered vs count}
ggplot(data = day.training.data, aes(x = cnt, y = casual)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_grid(cols = vars(season)) + 
  labs(title = "Casual Riders Influence on Total Count", x = "Count", y = "Casual Riders")
```

### Riders vs Temperature
Below is a scattered plot of the number of causal riders vs the raw temperature for each day in the span of the data parsed by workingday with a local polynomial regression line fit overtop. This plot provides insight on how many people spontaneous chose to ride based on the raw temperature of that day. 
```{r scatter - riders}
day.training.data$temp.indicator <- ifelse(day.training.data$temp < mean(day.training.data$temp), 0, 1)
day.training.data$temp.indicator <- as_factor(day.training.data$temp.indicator)
levels(day.training.data$temp.indicator) <- list("Low Temperature" = 0, "High Temperature" = 1)
ggplot(data = day.training.data, aes(x = temp.F, y = casual, color = workingday)) + 
  geom_point() + 
  geom_smooth() + 
  labs(title = "Casual Riders Based on Temperature", x = "Raw Temperature", y = "Number of Casual Riders")
```

### Riders vs. Hour vs. Month vs. Working Day
This boxplot below shows the count of riders for every month and every hour of the day. The color of the points indicate whether it was on a working day (1) or non-working day (0). We suspect for working days, there would be an obvious uptick around the morning and afternoon commute time. But for non-working days, the amount of riders shouldn't have an obvious pattern around those time frames. This plot shows us whether that's true or not. 
```{r scatter - count,hour,month,workingday, fig.width = 12}
count.df <- hour.training.data
levels(count.df$mnth) <- list(January = 1, February = 2, March = 3, April = 4, May = 5, June = 6, July = 7, August = 8, September = 9, October = 10, November = 11, December = 12)
ggplot(count.df, aes(x = hr, y = cnt)) +
  geom_point(aes(col = workingday)) +
  facet_wrap(vars(mnth)) + 
  labs(title = "Count of riders for every hour of every month",
       subtitle = "Specified by workday or non-workday",
       x = "Hour of the Day",
       y = "Count of Riders") +
  scale_color_discrete(name = "Working Day")
```

## Correlation Plot  
### Correlation between temp, atemp, hum, windspeed
This correlation plots show the correlation (positive or negative) between the 4 quantitative variables temperature, feeling temperature, humidity and wind speed. We suspect that there likely would be a high correlation between temperature and feeling temperature, and humidity might be inverse correlated with wind speed. This correlation can show us whether this is true. 
```{r correlation plot - temp,atemp,humidity,windspeed}
cor.variables <- hour.training.data %>% select(temp, atemp, hum, windspeed)
correlation <- cor(cor.variables, method = "spearman")
corrplot(correlation)
```

## Plots with GGally
The two GGally plots below will show whether there's any relationship between each of the variables. We run this plot on both the day and hour data.  
  
### Using Day Data
```{r GGally - day data}
subset.data.day <- data_frame(weathersit=day.training.data$weathersit, temp=day.training.data$temp, atemp=day.training.data$atemp,humidity=day.training.data$hum, windspeed=day.training.data$windspeed, casual=day.training.data$casual, registered=day.training.data$registered, total=day.training.data$cnt)
GGally::ggpairs(subset.data.day)
```

### Using Hour Data
```{r GGally - hour data}
subset.data.hr <- data_frame(weathersit=hour.training.data$weathersit, temp=hour.training.data$temp, atemp=hour.training.data$atemp,humidity=hour.training.data$hum, windspeed=hour.training.data$windspeed, casual=hour.training.data$casual, registered=hour.training.data$registered, total=hour.training.data$cnt)
GGally::ggpairs(subset.data.hr)
```


# Modeling  
## Linear Regression Model  
### What is Linear Regression Model
Linear regression is a type of modeling used to predict a response based on explanatory variables by fitting a linear equation to observed data. For simple linear regression using a single explanatory variable to predict a response variable the equation is ${Y}_{i} = \beta_{0} + \beta_{1}{x}_{i} + {E}_{i}$ where ${Y}_{i}$ is the response for the ${i}^{th}$ observation, ${x}_{i}$ is the value of the explanatory variable for the ${i}^{th}$ observation, $\beta_{0}$ is the y-intercept, $\beta_{1}$ is the slope, and ${E}_{i}$ is the error for the ${i}^{th}$ observation. Fitting a linear model to the observed dataset requires estimating the coefficients $\beta$ such that the error term ${E}_{i} = {Y}_{i} - \beta_{0} - \beta_{1}{x}_{i}$ is minimized. The most common way to minimize this term is through least-squares where we minimize the sum of squared residuals through $min_{\beta_{0},\beta_{1}}\sum_{i=1}^n ({y}_{i} - \beta_{0} - \beta_{1}{x}_{i})$. Simple linear regression can be extended in many ways to include:  
  
* higher order terms: ${Y}_{i} = \beta_{0} + \beta_{1}{x}_{i} + \beta_{2}{x}_{i}^{2} + {E}_{i}$  
* more explanatory variables: ${Y}_{i} = \beta_{0} + \beta_{1}{x}_{1i} + \beta_{2}{x}_{2i} + \beta_{3}{x}_{1i}{x}_{2i} + {E}_{i}$  
* more explanatory variables and higher order terms: ${Y}_{i} = \beta_{0} + \beta_{1}{x}_{1i} + \beta_{2}{x}_{2i} + \beta_{3}{x}_{1i}{x}_{2i} + \beta_{4}{x}_{1i}^{2} + \beta_{5}{x}_{2i}^{2} + {E}_{i}$  
  
In each of these linear regressions the model is still fit by minimizing the sum of squared errors. As the number of explanatory variables increase these regression models can become quite large, so it is best to compare different candidate models to see which provides the best fit of the data. Usually you would have some sort of subject matter knowledge to help select these candidate models by understanding which variables are related and which variables scientifically should be put in the model. Without subject matter knowledge you might select multiple candidate models and compare them using fit criteria such as AIC, BIC, AICc, Adjusted R-squared or Variance Inflation Factor (VIF). Alternatively, you may compare prediction error by splitting the data into a training and test set with a 80/20 split and fit the candidate models on the training set to predict the response of the test set. The model with the lowest RMSE should be considered to be the best fit as it minimized the error the best.  
  
### Picking predictors using AIC
First we want to select only the variables that we will use in our models, as variables such as record index, date are not useful to us. We will be using the un-normalized versions of temperature, feeling temperature, humidity and wind speed (instead of the normalized versions) because we want to standardize all numerical variables when running our models.  
Because on some days of the week holiday and working day both become 1 leveled factor variables and can cause issues in our modeling, so we will omit these 2 variables for those days of the week.
```{r filter out variables for modeling}
# keep only variables that are relevant to modeling
if.weekday <- hour.training.data %>% filter(weekday == params$weekday) %>% select(workingday) %>% unique() %>% nrow()
if.holiday <- hour.training.data %>% filter(weekday == params$weekday) %>% select(holiday) %>% unique() %>% nrow()

# use function to decide if a weekday has 1 factored levels
# if so we will not use these factors in the model 
get.data <- function(weekday, ...){
  if (if.weekday == 1 & if.holiday == 1) {
    hour.training.data2 <- hour.training.data %>% select(season, yr, mnth, hr, weathersit, temp.F, atemp.F, hum.unnormal, windspeed.unnormal, cnt)
  }
  else {
    hour.training.data2 <- hour.training.data %>% select(season, yr, mnth, hr, holiday, workingday, weathersit, temp.F, atemp.F, hum.unnormal, windspeed.unnormal, cnt)
  }
  hour.training.data2
}
hour.training.data2 <- get.data(params$weekday)
```

We will let the `step()` function to pick our models using the stepwise algorithm. We provide the `step()` function with 3 different linear models, first with just first order variables, second with squared terms and interactions, and third with first ordered variables and interactions. 
```{r pick aic predictors, results='hide'}
# aic using only 1st ordered terms
fit.aic <- step((lm(cnt ~ ., data = hour.training.data2, verbose = FALSE)), direction = "both")

# aic including squared terms and interactions
fit.aic2 <- step((lm(cnt ~ .^2 + I(temp.F^2) + I(atemp.F^2) + I(hum.unnormal^2) + I(windspeed.unnormal^2), data = hour.training.data2, verbose = FALSE)), direction = "both")

# aic using 1st order and interactions
fit.aic3 <- step((lm(cnt ~.^2, data = hour.training.data2, verbose = FALSE)), direction = "both")
```  

### Modeling using AIC picked predictor
For the first linear regression model we run, we will pick predictors based on our intuition. We expect the feeling temperature would be highly correlated with the actual temperature, and because wind speed and humidity could also be correlated with feeling temperature, so we only kept temperature as a predictor in the model and will not include feeling temperature. We also did not include holiday in the model because it has some redundant information to the working day variable.  
```{r linear regression - plain}
# use all predictors except atemp and holiday
set.seed(7)
fit.mlr0 <- train(cnt ~ season + yr + mnth + hr + workingday + weathersit + temp.F + hum.unnormal + windspeed.unnormal,
                  data = hour.training.data,
                  method = "lm",
                  preProcess = c("center", "scale"),
                  trControl = trainControl(method = "cv", number = 10))
fit.mlr0

# Examine performance of this multiple linear regression model on the test data after prediction
predict.mlr0 <- postResample(predict(fit.mlr0, newdata = hour.test.data), obs = hour.test.data$cnt)
```

The next three linear regression models are fit using the predictors picked by the three step functions. These models include different number of predictors in different complexity, so we will see which models will produce the best prediction in the end. 
```{r linear regression - aic1}
# use aic predictors (1st ordered terms)
set.seed(7)
fit.mlr1 <- train(fit.aic$terms,
                  data = hour.training.data2,
                  method = "lm",
                  preProcess = c("center", "scale"),
                  trControl = trainControl(method = "cv", number = 10))
fit.mlr1

# variables used in fit
fit.aic

# Examine performance of this multiple linear regression model on the test data after prediction
predict.mlr1 <- postResample(predict(fit.mlr1, newdata = hour.test.data), obs = hour.test.data$cnt)
```
```{r linear regression - aic2}
# use aic predictors (2nd ordered terms and interactions)
set.seed(7)
fit.mlr2 <- train(fit.aic2$terms,
                  data = hour.training.data2,
                  method = "lm",
                  preProcess = c("center", "scale"),
                  trControl = trainControl(method = "cv", number = 10))
fit.mlr2

# variables used in fit
fit.aic2

# Examine performance of this multiple linear regression model on the test data after prediction
predict.mlr2 <- postResample(predict(fit.mlr2, newdata = hour.test.data), obs = hour.test.data$cnt)
```
```{r linear regression - aic3}
# use aic predictors (1st order and interactions)
set.seed(7)
fit.mlr3 <- train(fit.aic3$terms,
                  data = hour.training.data2,
                  method = "lm",
                  preProcess = c("center", "scale"),
                  trControl = trainControl(method = "cv", number = 10))
fit.mlr3

# variables used in fit
fit.aic3

# Examine performance of this multiple linear regression model on the test data after prediction
predict.mlr3 <- postResample(predict(fit.mlr3, newdata = hour.test.data), obs = hour.test.data$cnt)
```

## Ensemble Tree Model  
### Random Forest Model  
#### What is Random Forest Model?
The random forest model is a type of tree based method where we create multiple trees from bootstrap samples of the data and then average the results. This process is done by first creating a bootstrap sample of the data and then training a tree on this sample where we call the prediction for a given set of $x$ values $\hat{y}^{*1}(x)$. This process is then repeated a $B$ number of times to obtain $\hat{y}^{*j}(x), j = 1,...,B$. The final prediction is the average of these predictions $\hat{y}(x) = \frac{1}{B}\sum_{j=1}^{B}\hat{y}^{*j}(x)$. For each of these bootstrap sample/tree fits a random subset of predictors is chosen becasue if a really strong predictor exists, every bootstrap tree will probably use it as the first split. By selecting a subset of predictors, a good predictor or two won't dominate the tree fits. The number of predictors for a random forest regression tree is usually $m = p/3$ where $m$ is the random predictors chosen and $p$ is the full set of possible predictors. Cross-validation can also be used to select these random predictors as we did in our random forest model.  
  
We first fit a random forest model using default tuning parameters, which produced a result with very large mtry values.
```{r models - random forest default}
# Fit random forest model
set.seed(7)
fit.random.forest.trial <- train(fit.aic$terms,
                           data = hour.training.data2,
                           method = "rf",
                           preProcess = c("center", "scale"),
                           trControl = trainControl(method = "cv", number = 10),
                           verbose = FALSE)
fit.random.forest.trial
```

We can manually tune our parameters to only include 1 to the number of predictors. We will use the result from this model to do predictions. 
```{r models - random forest mannual}
set.seed(7)
fit.random.forest <- train(fit.aic$terms,
                           data = hour.training.data2,
                           method = "rf",
                           preProcess = c("center", "scale"),
                           trControl = trainControl(method = "cv", number = 10),
                           tuneGrid = data.frame(mtry = 1:(ncol(hour.training.data2) -1)))
fit.random.forest

# Examine performance of random forest model on the test data after prediction
predict.rf <- postResample(predict(fit.random.forest, newdata = hour.test.data), obs = hour.test.data$cnt)
```

### Boosted Tree Model  
#### What is Boosted Tree Model?
The boosted tree model is a type of tree based method where we grow our trees in a sequential manner, each tree we create will be based off the previous tree so we can update our prediction as we go. For example, we'd fit our model and get a prediction, then create a new model based off the previous, update the prediction on this new model, and we'd repeat this process until we decide to stop. Boosted tree model slowly trains the trees to ensure we don't overfit to our training data. How this is actually done is we create new residuals based off `observed - new predictions`, fit a tree to those residuals to get new predictions $\hat{y}$, then update our predictions again by a scaled down version of the new predictions $\lambda \hat{y}^{b}$ (here $\lambda$ is the growth rate tuning parameter, which keeps us from growing our predictions too quickly). We repeat this process a total of `B` times. We can use cross validation to select what $\lambda$, `d` and `B` should be. Formula used to update predictions is $\hat{y} = \hat{y} + \lambda \hat{y}^{b}$.  
  
For the boosted tree model, we first let the model pick default tuning parameters, from the result we will further fine tune the parameters to see if we can get even better results.   
```{r boosted tree - default}
set.seed(7)
fit.boosted.trial <- train(fit.aic$terms,
                     data = hour.training.data2,
                     method = "gbm",
                     preProcess = c("center", "scale"),
                     trControl = trainControl(method = "cv", number = 10),
                     verbose = FALSE)
fit.boosted.trial
```

```{r boosted tree - manual}
set.seed(7)
fit.boosted <- train(fit.aic$terms,
                     data = hour.training.data2,
                     method = "gbm",
                     preProcess = c("center", "scale"),
                     trControl = trainControl(method = "cv", number = 10),
                     verbose = FALSE,
                     tuneGrid = expand.grid(interaction.depth = c(3:10),
                                            n.trees = (3:10)*50,
                                            shrinkage = 0.1,
                                            n.minobsinnode = 10))
fit.boosted

# Examine performance of boosted tree model on the test data after prediction
predict.boosted <- postResample(predict(fit.boosted, newdata = hour.test.data), obs = hour.test.data$cnt)
```  
  
# Comparison
We compare all 6 models on the test set and see which model produced the lowest root mean squared error value, which indicate that model out of the 6 had the best prediction.
```{r comparison}
compare.rmse <- data.frame(predict.mlr0, 
                           predict.mlr1,
                           predict.mlr2,
                           predict.mlr3,
                           predict.rf,
                           predict.boosted)
colnames(compare.rmse) <- c("mlr manual", "mlr aic1", "mlr aic2", "mlr aic3", "random forest", "boosted tree")
compare.rmse
```

```{r pick winner}
min.compare.rmse <- min(compare.rmse["RMSE",])
min.test <- compare.rmse["RMSE",] == min.compare.rmse
paste0("After comparing all models on the test set, the model with the best prediction (lowest root MSE value) is the ", colnames(compare.rmse)[min.test], " model.")
```
